# Auto-generated llama-swap configuration
# Models directory: /Users/badfy17g/.clara/llama-models
healthCheckTimeout: 30
logLevel: info

models:
  "nomic-embed-text-v1-5":
    proxy: "http://127.0.0.1:9998"
    cmd: |
      "/Users/badfy17g/ClaraVerse/electron/llamacpp-binaries/darwin-arm64/llama-server"
      -m "/Users/badfy17g/.clara/llama-models/nomic-embed-text-v1.5.Q4_0.gguf"
      --port 9998 --jinja --n-gpu-layers 60 --pooling mean --embeddings --threads 1 --batch-size 512 --ubatch-size 512 --keep 2048 --defrag-thold 0.1 --mlock --parallel 1 --flash-attn
    env:
      - "DYLD_LIBRARY_PATH=/Users/badfy17g/ClaraVerse/electron/llamacpp-binaries/darwin-arm64:"
    ttl: 300

groups:
  "embedding_models":
    # Allow multiple embedding models to run together
    swap: false
    # Don't unload other groups when embedding models start
    exclusive: false
    # Prevent other groups from unloading embedding models
    persistent: true
    members:
      - "nomic-embed-text-v1-5"
