# Auto-generated llama-swap configuration
# Models directory: C:\Users\prave\.clara\llama-models
healthCheckTimeout: 30
logLevel: info

models:
  "gemma3:12b":
    proxy: "http://127.0.0.1:9999"
    cmd: |
      "C:\Users\prave\Desktop\ClaraVerse\electron\llamacpp-binaries\win32-x64\llama-server.exe"
      -m "C:\Users\prave\.clara\llama-models\gemma-3-12b-it-Q4_K_M.gguf"
      --port 9999 --jinja --n-gpu-layers 50 --threads 8 --ctx-size 16384 --batch-size 256 --ubatch-size 256 --keep 2048 --defrag-thold 0.1 --mlock --parallel 4 --flash-attn --cont-batching
    ttl: 300

  "llama3.2:3b":
    proxy: "http://127.0.0.1:9999"
    cmd: |
      "C:\Users\prave\Desktop\ClaraVerse\electron\llamacpp-binaries\win32-x64\llama-server.exe"
      -m "C:\Users\prave\.clara\llama-models\Llama-3.2-3B-Instruct-IQ4_XS.gguf"
      --port 9999 --jinja --n-gpu-layers 50 --threads 8 --ctx-size 16384 --batch-size 256 --ubatch-size 256 --keep 2048 --defrag-thold 0.1 --mlock --parallel 4 --flash-attn --cont-batching
    ttl: 300

  "moondream:2":
    proxy: "http://127.0.0.1:9999"
    cmd: |
      "C:\Users\prave\Desktop\ClaraVerse\electron\llamacpp-binaries\win32-x64\llama-server.exe"
      -m "C:\Users\prave\.clara\llama-models\moondream2-text-model-f16.gguf"
      --port 9999 --jinja --n-gpu-layers 50 --threads 8 --ctx-size 16384 --batch-size 256 --ubatch-size 256 --keep 2048 --defrag-thold 0.1 --mlock --parallel 4 --flash-attn --cont-batching
    ttl: 300

  "qwen3:14b":
    proxy: "http://127.0.0.1:9999"
    cmd: |
      "C:\Users\prave\Desktop\ClaraVerse\electron\llamacpp-binaries\win32-x64\llama-server.exe"
      -m "C:\Users\prave\.clara\llama-models\Qwen3-14B-UD-Q4_K_XL.gguf"
      --port 9999 --jinja --n-gpu-layers 50 --threads 8 --ctx-size 16384 --batch-size 256 --ubatch-size 256 --keep 2048 --defrag-thold 0.1 --mlock --parallel 4 --flash-attn --cont-batching
    ttl: 300

  "tinyllama:1.1b":
    proxy: "http://127.0.0.1:9999"
    cmd: |
      "C:\Users\prave\Desktop\ClaraVerse\electron\llamacpp-binaries\win32-x64\llama-server.exe"
      -m "C:\Users\prave\.clara\llama-models\tinyllama-1.1b-chat-v1.0.Q4_0.gguf"
      --port 9999 --jinja --n-gpu-layers 50 --threads 8 --ctx-size 16384 --batch-size 256 --ubatch-size 256 --keep 2048 --defrag-thold 0.1 --mlock --parallel 4 --flash-attn --cont-batching
    ttl: 300

  "qwen2.5:14b":
    proxy: "http://127.0.0.1:9999"
    cmd: |
      "C:\Users\prave\Desktop\ClaraVerse\electron\llamacpp-binaries\win32-x64\llama-server.exe"
      -m "C:\Users\prave\Documents\qwen2.5-coder-14b-instruct-q4_k_m.gguf"
      --port 9999 --jinja --n-gpu-layers 50 --threads 8 --ctx-size 16384 --batch-size 256 --ubatch-size 256 --keep 2048 --defrag-thold 0.1 --mlock --parallel 4 --flash-attn --cont-batching
    ttl: 300

  "uigen-t3-preview-q8-0":
    proxy: "http://127.0.0.1:9999"
    cmd: |
      "C:\Users\prave\Desktop\ClaraVerse\electron\llamacpp-binaries\win32-x64\llama-server.exe"
      -m "C:\Users\prave\Documents\uigen-t3-preview-q8_0.gguf"
      --port 9999 --jinja --n-gpu-layers 50 --threads 8 --ctx-size 16384 --batch-size 256 --ubatch-size 256 --keep 2048 --defrag-thold 0.1 --mlock --parallel 4 --flash-attn --cont-batching
    ttl: 300

groups:
  "default_group":
    swap: true
    exclusive: true
    members:
      - "gemma3:12b"
      - "llama3.2:3b"
      - "moondream:2"
      - "qwen3:14b"
      - "tinyllama:1.1b"
      - "qwen2.5:14b"
      - "uigen-t3-preview-q8-0"
